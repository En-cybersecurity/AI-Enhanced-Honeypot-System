# -*- coding: utf-8 -*-
"""Ransomware_detection - Updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CiamT6E6rqIScx1dMxBIQx3nvkf6csor
"""

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
import warnings
from sklearn.svm import SVC
from numpy import mean
from numpy import std
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score

warnings.filterwarnings(action="ignore")

import os
for dirname, _, filenames in os.walk('/content/data_file.csv'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""## Preprocessing"""

# Open the datasets:
df = pd.read_csv("/content/data_file.csv")
# Delete the useless columns:
cols_to_drop = ['FileName', 'md5Hash']
df = df.drop(columns=cols_to_drop)
# Replace the value repeated: MD5HASH - DebugSize - MajorOSVersion - BitcoinAddresses - NumberOfSections - SizeOfStackReserve
columns = ["Machine", "DebugSize", "NumberOfSections", "SizeOfStackReserve", "MajorOSVersion", "BitcoinAddresses"]
for col in columns:
    df[col] = df[col].astype('category')
    df[col] = df[col].cat.codes
# Delete all the duplicated rows:
df.drop_duplicates(keep='last')
# Save the new datasets into a new csv file:
df.to_csv("df_clear.csv")

# Take a look at the data after processing
df = pd.read_csv("df_clear.csv")
df.head()

# DataFrame information
df.info()

# Describe dataframe
df.describe()

"""## Exploratory Data Analysis"""

# Analyze the distribution of the 'Benign' label (Malicious or Benign files)
plt.figure(figsize=(6,4))
sns.countplot(data=df, x='Benign')
plt.title('Distribution of Benign Files')
plt.show()

# Analyze correlations between numeric columns
plt.figure(figsize=(12,8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

#Distribution of key numeric columns
numeric_columns = ['DebugSize', 'ExportSize', 'MajorImageVersion', 'MajorOSVersion', 'SizeOfStackReserve', 'ResourceSize']
df[numeric_columns].hist(bins=20, figsize=(14,10))
plt.suptitle('Distributions of Numeric Features')
plt.show()

# Analyze distribution of Machine types (architecture)
plt.figure(figsize=(8,6))
sns.countplot(data=df, x='Machine')
plt.title('Distribution of Machine Types (Architecture)')
plt.show()

# Correlation of features with the 'Benign' label
plt.figure(figsize=(10,6))
sns.barplot(x=df.corr()['Benign'].index, y=df.corr()['Benign'].values)
plt.xticks(rotation=90)
plt.title('Correlation of Features with Benign Label')
plt.show()

"""### Transforming Dataset into chunks for training models"""

# Transorming dataset into subset
df_subset = df.iloc[5000:55000, :]

# Display the shape of the combined dataset to verify
print("Combined dataset shape:", df_subset.shape)

# Transform into Lists:
X = df_subset.iloc[:, 1:-1].values
y = df_subset.iloc[:, -1].values
print("The features (Machine ... Bitcoin@) : ")
print(X)
print("Target vector (Benign) : ")
print(y)

# Split data into training and testing sets
# 20% testing and 80% train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Checking the shape of the training and test sets
print('Shape of the training input data:', X_train.shape)
print('Shape of the training output data:', y_train.shape)
print('Shape of the test input data:', X_test.shape)
print('Shape of the test output data:', y_test.shape)

"""## Support Vector Machine"""

# Create a Support Vector Machine (SVM) Classifier
svm = SVC(kernel='rbf', random_state=0)

# Fit the SVM Classifier to the training data
svm.fit(X_train, y_train)
# Predict the classes of the testing set using SVM
y_pred_svm = svm.predict(X_test)
# Print the accuracy of the model
print("Accuracy:", svm.score(X_test, y_test))

!pip install joblib
# Save the trained model to a file
svm_filename = 'svm_classifier_model'
import joblib
joblib.dump(svm, svm_filename)

print(f"Model saved to {svm_filename}")

# Confusion matrix and analysis for Support Vector Machine
cm_svm = confusion_matrix(y_test, y_pred_svm)
print("Confusion Matrix for Support Vector Machine:")
print(cm_svm)
plt.figure()
plot_confusion_matrix(cm_svm, figsize=(8, 6))
plt.title("Confusion Matrix - Support Vector Machine")
plt.show()

"""## XGBoost Classifier"""

#Create the XGBoost Classifier
xgboost = XGBClassifier()
# evaluate the model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
n_scores = cross_val_score( xgboost, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))

# fit the model on the whole dataset
xgboost.fit(X_train, y_train)
y_pred_xg = xgboost.predict(X_test)

# Save the trained model to a file
xgboost_filename = 'XGBoost_classifier_model'
import joblib
joblib.dump(xgboost, xgboost_filename)

print(f"Model saved to {xgboost_filename}")

# Confusion matrix and analysis for XGBoost Classifier
cm_xg = confusion_matrix(y_test, y_pred_xg)
print("Confusion Matrix for XGBoost Classifier:")
print(cm_xg)
plt.figure()
plot_confusion_matrix(cm_xg, figsize=(8, 6))
plt.title("Confusion Matrix - XGBoost Classifier")
plt.show()

"""## Random Forest Classifier"""

# Create a Random Forest Classifier with 300 trees
rf = RandomForestClassifier(n_estimators=300,random_state=0)

# Fit the Random Forest Classifier to the training data
rf.fit(X_train, y_train)

# Predict the classes of the testing set
y_pred_rf = rf.predict(X_test)

# Print the accuracy of the model
print("Accuracy:", rf.score(X_test, y_test))

# Save the trained model to a file
rf_filename = 'RandomForest_classifier_model'
import joblib
joblib.dump(rf, rf_filename)

print(f"Model saved to {rf_filename}")

# Confusion matrix and analysis for Random Forest Classifier
cm_rt = confusion_matrix(y_test, y_pred_rf)
print("Confusion Matrix for Random Forest Classifier:")
print(cm_rt)
plt.figure()
plot_confusion_matrix(cm_rt, figsize=(8, 6))
plt.title("Confusion Matrix - Random Forest Classifier")
plt.show()

"""## Artificial Neural Network (ANN) Classifier"""

# Create an Artificial Neural Network (ANN) Classifier
ann = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=0)
# Fit the ANN Classifier to the training data
ann.fit(X_train, y_train)
# Predict the classes of the testing set using ANN
y_pred_ann = ann.predict(X_test)
# Print the accuracy of the model
print("Accuracy:", ann.score(X_test, y_test))

# Save the trained model to a file
ann_filename = 'ANNclassifier_model'
import joblib
joblib.dump(ann, ann_filename)

print(f"Model saved to {ann_filename}")

# Confusion matrix and analysis for Artificial Neural Network
cm_ann = confusion_matrix(y_test, y_pred_ann)
print("Confusion Matrix for Artificial Neural Network:")
print(cm_ann)
plt.figure()
plot_confusion_matrix(cm_ann, figsize=(8, 6))
plt.title("Confusion Matrix - Artificial Neural Network")
plt.show()

"""## Testing Saved Models on Unseen Dataset

### Testing SVM
"""

# Select rows from 1 to 5000
df_first_part = df.iloc[:5000, :]

# Select rows from 55001 to the end
df_second_part = df.iloc[55000:, :]

# Combine the two parts into one dataframe
df_combined = pd.concat([df_first_part, df_second_part])

# Display the shape of the combined dataset to verify
print("Combined dataset shape:", df_combined.shape)

# Transform the test set into feature vectors and target labels
X_test = df_combined.iloc[:, 1:-1].values
y_test = df_combined.iloc[:, -1].values

# Load the saved model
model_filename = 'svm_classifier_model'
loaded_model = joblib.load(model_filename)

# Predict using the loaded model
y_pred_test = loaded_model.predict(X_test)

# Print the predictions and accuracy
print("Predictions on test data:", y_pred_test)
print("Accuracy on test data:", loaded_model.score(X_test, y_test))

"""### Testing XGBoost"""

# Load the saved model
model_filename = 'XGBoost_classifier_model'
loaded_model = joblib.load(model_filename)

# Predict using the loaded model
y_pred_test = loaded_model.predict(X_test)

# Print the predictions and accuracy
print("Predictions on test data:", y_pred_test)
print("Accuracy on test data:", loaded_model.score(X_test, y_test))

"""## Testing RandomForest Classifier"""

# Load the saved model
model_filename = 'RandomForest_classifier_model'
loaded_model = joblib.load(model_filename)

# Predict using the loaded model
y_pred_test = loaded_model.predict(X_test)

# Print the predictions and accuracy
print("Predictions on test data:", y_pred_test)
print("Accuracy on test data:", loaded_model.score(X_test, y_test))

"""## Testing Artifical Neural Network"""

# Load the saved model
model_filename = 'ANNclassifier_model'
loaded_model = joblib.load(model_filename)

# Predict using the loaded model
y_pred_test = loaded_model.predict(X_test)

# Print the predictions and accuracy
print("Predictions on test data:", y_pred_test)
print("Accuracy on test data:", loaded_model.score(X_test, y_test))

